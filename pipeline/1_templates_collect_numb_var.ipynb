{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current output directory: '../out/poslog'\n"
     ]
    }
   ],
   "source": [
    "# GENERIC FIRST CELL FOR DEVELOPING A NEW METHODOLOGY\n",
    "\n",
    "# Define an expressive name for the methodology, that will be used to create the output directory\n",
    "# The complete path to the output directory will be provided in OUT_DIR\n",
    "METHODOLOGY_NAME='poslog/'\n",
    "\n",
    "# Set to number of directories below project root if the notebook is in a subdirectory of the project, so you can use relative paths\n",
    "SUBDIR_LEVEL = 1\n",
    "if SUBDIR_LEVEL>0:\n",
    "    import sys \n",
    "    new_path = '../'*SUBDIR_LEVEL\n",
    "    if new_path not in sys.path:\n",
    "        sys.path.append(new_path)\n",
    "\n",
    "import os\n",
    "OUT_DIR = os.path.relpath(os.path.join(os.getcwd(), '../'*SUBDIR_LEVEL, 'out', METHODOLOGY_NAME))\n",
    "if not os.path.exists(OUT_DIR):\n",
    "    os.makedirs(OUT_DIR)\n",
    "print(f\"Current output directory: '{OUT_DIR}'\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input directory: ../data/\n",
      "Output file: ../out/poslog/1_examples_10000_each_seed-42_numb_var.csv\n"
     ]
    }
   ],
   "source": [
    "N=10000\n",
    "SEED=42\n",
    "\n",
    "OUTPUT_FILE=os.path.join(OUT_DIR, '1_examples_'+str(N)+'_each_seed-'+str(SEED)+'_numb_var.csv')\n",
    "\n",
    "# Files for each dataset with all templates for NumbVar\n",
    "OUTPUT_FILE_ALL_TEMPLATES='numb_var_templates_{dataset}.txt'\n",
    "# Files for each 10k sub-dataset \n",
    "OUTPUT_FILE_N_LINES='selection_{dataset}_{N}_each_seed-{SEED}.pkl'\n",
    "\n",
    "INPUT_DIR = '../'*SUBDIR_LEVEL+'data/'\n",
    "\n",
    "print(f'Input directory: {INPUT_DIR}')\n",
    "print(f'Output file: {OUTPUT_FILE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Datasets to one File each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_log_files_in_folder(input_dir:str, collection_file:str, suffix:str='.log', exclude:list[str]=[])->None:\n",
    "    # make sure not to collect collection file itself\n",
    "    basename = os.path.basename(collection_file)\n",
    "    if not basename in exclude:\n",
    "        exclude.append(basename)\n",
    "\n",
    "    # list all files alphabetically\n",
    "    for file in sorted(os.listdir(input_dir)):\n",
    "        # if is log file\n",
    "        if file.endswith(suffix):\n",
    "            if file in exclude:\n",
    "                continue\n",
    "            with open(collection_file, \"a\") as output_file:\n",
    "                with open(os.path.join(input_dir, file), \"r\") as input_file:\n",
    "                    output_file.write(input_file.read())\n",
    "                    \n",
    "def collect_all_log_files_in_subfolders(input_dir:str, collection_file:str, suffix:str='.log')->None:\n",
    "    #for each folder in the directory collect all files of type .log to one huge file\n",
    "    for folder in sorted(os.listdir(input_dir)):\n",
    "        subfolder=os.path.join(input_dir, folder)\n",
    "        # if is folder open and list all files\n",
    "        if os.path.isdir(subfolder):\n",
    "            collect_all_log_files_in_folder(subfolder, collection_file, suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect Hadoop dataset\n",
    "collected_file=os.path.join(INPUT_DIR,'Hadoop/Hadoop_collected.log')\n",
    "dir_with_logs=os.path.join(INPUT_DIR,'Hadoop/')\n",
    "\n",
    "# skip if file exists\n",
    "if os.path.exists(collected_file):\n",
    "    print(f\"File {collected_file} already exists, skipping\")\n",
    "else:\n",
    "    collect_all_log_files_in_subfolders(dir_with_logs, collected_file)\n",
    "    print(f\"Collected all log files in {dir_with_logs} to {collected_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect Spark dataset\n",
    "collected_file=os.path.join(INPUT_DIR,'Spark/Spark_collected.log')\n",
    "dir_with_logs=os.path.join(INPUT_DIR,'Spark/')\n",
    "\n",
    "# skip if file exists\n",
    "if os.path.exists(collected_file):\n",
    "    print(f\"File {collected_file} already exists, skipping\")\n",
    "else:\n",
    "    collect_all_log_files_in_subfolders(dir_with_logs, collected_file)\n",
    "    print(f\"Collected all log files in {dir_with_logs} to {collected_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect OpenStack dataset\n",
    "collected_file=os.path.join(INPUT_DIR,'OpenStack/OpenStack_collected.log')\n",
    "dir_with_logs=os.path.join(INPUT_DIR,'OpenStack/')\n",
    "\n",
    "# skip if file exists\n",
    "if os.path.exists(collected_file):\n",
    "    print(f\"File {collected_file} already exists, skipping\")\n",
    "else:\n",
    "    collect_all_log_files_in_folder(dir_with_logs, collected_file)\n",
    "    print(f\"Collected all log files in {dir_with_logs} to {collected_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect all Templates with NumbVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'HDFS': {\n",
    "#        'log_file': 'HDFS/HDFS_2k.log',\n",
    "        \"log_file\": \"HDFS_v1/HDFS.log\",\n",
    "        'log_format': '<Date> <Time> <Pid> <Level> <Component>: <Content>',\n",
    "        'regex': [r'blk_-?\\d+', r'(\\d+\\.){3}\\d+(:\\d+)?'],\n",
    "        \"lines\": 11175629,\n",
    "        },\n",
    "\n",
    "    'Hadoop': {\n",
    "#        'log_file': 'Hadoop/Hadoop_2k.log',\n",
    "        \"log_file\": \"Hadoop/Hadoop_collected.log\",\n",
    "        'log_format': '<Date> <Time> <Level> \\[<Process>\\] <Component>: <Content>',\n",
    "        'regex': [r'(\\d+\\.){3}\\d+', r'SUCCESS_ CONTAINER_ CLEANUP'],\n",
    "        \"lines\": 394308,\n",
    "        },\n",
    "\n",
    "    'Spark': {\n",
    "#        'log_file': 'Spark/Spark_2k.log',\n",
    "        \"log_file\": \"Spark/Spark_collected.log\",\n",
    "        'log_format': '<Date> <Time> <Level> <Component>: <Content>',\n",
    "        'regex': [r'(\\d+\\.){3}\\d+', r'\\b[KGTM]?B\\b', r'([\\w-]+\\.){2,}[\\w-]+'],\n",
    "        \"lines\": 33236604,\n",
    "        },\n",
    "\n",
    "    'Zookeeper': {\n",
    "#        'log_file': 'Zookeeper/Zookeeper_2k.log',\n",
    "        \"log_file\": \"Zookeeper/Zookeeper.log\",\n",
    "        'log_format': '<Date> <Time> - <Level>  \\[<Node>:<Component>@<Id>\\] - <Content>',\n",
    "        'regex': [r'(/|)(\\d+\\.){3}\\d+(:\\d+)?'],\n",
    "        \"lines\": 74380,\n",
    "        },\n",
    "\n",
    "    'BGL': {\n",
    "#        'log_file': 'BGL/BGL_2k.log',\n",
    "        \"log_file\": \"BGL/BGL.log\",\n",
    "        'log_format': '<Label> <Timestamp> <Date> <Node> <Time> <NodeRepeat> <Type> <Component> <Level> <Content>',\n",
    "        'regex': [r'core\\.\\d+'],\n",
    "        \"lines\": 4747963,\n",
    "        },\n",
    "\n",
    "    'HPC': {\n",
    "#        'log_file': 'HPC/HPC_2k.log',\n",
    "        \"log_file\": \"HPC/HPC.log\",\n",
    "        'log_format': '<LogId> <Node> <Component> <State> <Time> <Flag> <Content>',\n",
    "        'regex': [],\n",
    "        \"lines\": 433490,\n",
    "        },\n",
    "\n",
    "    'Thunderbird': {\n",
    "#        'log_file': 'Thunderbird/Thunderbird_2k.log',\n",
    "        \"log_file\": \"Thunderbird/Thunderbird.log\",\n",
    "        'log_format': '<Label> <Timestamp> <Date> <User> <Month> <Day> <Time> <Location> <Component>(\\[<PID>\\])?: <Content>',\n",
    "        'regex': [r'(\\d+\\.){3}\\d+'],\n",
    "        \"lines\": 211212192,\n",
    "        },\n",
    "\n",
    "    'Windows': {\n",
    "#        'log_file': 'Windows/Windows_2k.log',\n",
    "        \"log_file\": \"Windows/Windows.log\",\n",
    "        'log_format': '<Date> <Time>, <Level>                  <Component>    <Content>',\n",
    "        'regex': [r'0x.*?\\s'],\n",
    "        \"lines\": 114608388,\n",
    "        },\n",
    "\n",
    "    'Linux': {\n",
    "#        'log_file': 'Linux/Linux_2k.log',\n",
    "        \"log_file\": \"Linux/Linux.log\",\n",
    "        'log_format': '<Month> <Date> <Time> <Level> <Component>(\\[<PID>\\])?: <Content>',\n",
    "        'regex': [r'(\\d+\\.){3}\\d+', r'\\d{2}:\\d{2}:\\d{2}',r'J([a-z]{2})'],\n",
    "        \"lines\": 25567,\n",
    "        },\n",
    "\n",
    "    'Android': {\n",
    "#        'log_file': 'Android/Android_2k.log',\n",
    "        \"log_file\": \"Android_v1/Android.log\",\n",
    "        'log_format': '<Date> <Time>  <Pid>  <Tid> <Level> <Component>: <Content>',\n",
    "        'regex': [r'(/[\\w-]+)+', r'([\\w-]+\\.){2,}[\\w-]+', r'\\b(\\-?\\+?\\d+)\\b|\\b0[Xx][a-fA-F\\d]+\\b|\\b[a-fA-F\\d]{4,}\\b'],\n",
    "        \"lines\": 1555005,\n",
    "        },\n",
    "\n",
    "    'HealthApp': {\n",
    "#        'log_file': 'HealthApp/HealthApp_2k.log',\n",
    "        \"log_file\": \"HealthApp/HealthApp.log\",\n",
    "        'log_format': '<Time>\\|<Component>\\|<Pid>\\|<Content>',\n",
    "        'regex': [],\n",
    "        \"lines\": 253395,\n",
    "        },\n",
    "\n",
    "    'Apache': {\n",
    "#        'log_file': 'Apache/Apache_2k.log',\n",
    "        \"log_file\": \"Apache/Apache.log\",\n",
    "        'log_format': '\\[<Time>\\] \\[<Level>\\] <Content>',\n",
    "        'regex': [r'(\\d+\\.){3}\\d+'],\n",
    "        \"lines\": 56482,\n",
    "        },\n",
    "    'Proxifier': {\n",
    "#        'log_file': 'Proxifier/Proxifier_2k.log',\n",
    "        \"log_file\": \"Proxifier/Proxifier.log\",\n",
    "        'log_format': '\\[<Time>\\] <Program> - <Content>',\n",
    "        'regex': [r'<\\d+\\ssec', r'([\\w-]+\\.)+[\\w-]+(:\\d+)?', r'\\d{2}:\\d{2}(:\\d{2})*', r'[KGTM]B'],\n",
    "        \"lines\": 21329,\n",
    "    },\n",
    "\n",
    "    'OpenSSH': {\n",
    "#        'log_file': 'OpenSSH/OpenSSH_2k.log',\n",
    "        \"log_file\": \"SSH/SSH.log\",\n",
    "        'log_format': '<Date> <Day> <Time> <Component> sshd\\[<Pid>\\]: <Content>',\n",
    "        'regex': [r'(\\d+\\.){3}\\d+', r'([\\w-]+\\.){2,}[\\w-]+'],\n",
    "        \"lines\": 655147,\n",
    "        },\n",
    "\n",
    "    'OpenStack': {\n",
    "#        'log_file': 'OpenStack/OpenStack_2k.log',\n",
    "        \"log_file\": \"OpenStack/OpenStack_collected.log\",\n",
    "        'log_format': '<Logrecord> <Date> <Time> <Pid> <Level> <Component> \\[<ADDR>\\] <Content>',\n",
    "        'regex': [r'((\\d+\\.){3}\\d+,?)+', r'/.+?\\s ', r'\\d+'],\n",
    "        \"lines\": 207820,\n",
    "        },\n",
    "\n",
    "    'Mac': {\n",
    "#        'log_file': 'Mac/Mac_2k.log',\n",
    "        \"log_file\": \"Mac/Mac.log\",\n",
    "        'log_format': '<Month>  <Date> <Time> <User> <Component>\\[<PID>\\]( \\(<Address>\\))?: <Content>',\n",
    "        'regex': [r'([\\w-]+\\.){2,}[\\w-]+'],\n",
    "        \"lines\": 117283,\n",
    "        },\n",
    "}\n",
    "\n",
    "import re\n",
    "# from Brain (same as Drain)\n",
    "def generate_logformat_regex(logformat:str)->tuple[list[str], re.Pattern]:\n",
    "    \"\"\" Function to generate regular expression to split log messages\n",
    "    \"\"\"\n",
    "    headers = []\n",
    "    splitters = re.split(r'(<[^<>]+>)', logformat)\n",
    "    regex = ''\n",
    "    for k in range(len(splitters)):\n",
    "        if k % 2 == 0:\n",
    "            splitter = re.sub(' +', '\\\\\\s+', splitters[k])\n",
    "            regex += splitter\n",
    "        else:\n",
    "            header = splitters[k].strip('<').strip('>')\n",
    "            regex += '(?P<%s>.*?)' % header\n",
    "            # regex += \"(?P<%s>[\\s\\S]*?)\" % header\n",
    "            headers.append(header)\n",
    "    regex = re.compile('^' + regex + '$')\n",
    "    return headers, regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect **All** Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import re\n",
    "from util.logparsing import NumbVar\n",
    "\n",
    "def skip_line(line:str)->bool:\n",
    "    if content is None:\n",
    "        return True\n",
    "    if line.strip() == '':\n",
    "        return True\n",
    "    if line.lower() in ['none', 'null', 'nan', 'n/a', 'na']:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "template_files={}\n",
    "\n",
    "for dataset, config in config.items():\n",
    "    output_file=os.path.join(OUT_DIR,OUTPUT_FILE_ALL_TEMPLATES.format(dataset=dataset))\n",
    "    template_files[dataset]=output_file\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"File {output_file} already exists, skipping\")\n",
    "        continue\n",
    "\n",
    "    log_file = os.path.join(INPUT_DIR, config[\"log_file\"])\n",
    "    log_format:str=config[\"log_format\"]\n",
    "    headers: list[str]\n",
    "    format_regex: re.Pattern\n",
    "    headers, format_regex = generate_logformat_regex(log_format)\n",
    "\n",
    "    lines_count=int(config[\"lines\"])\n",
    "\n",
    "    print(f\"\\nProcessing {dataset} ({log_file})\")\n",
    "\n",
    "    def get_content_from_line(cur_line:str)->str:\n",
    "        try:\n",
    "            match = format_regex.search(cur_line.strip())\n",
    "            log_line_dict = {header:match.group(header) for header in headers}\n",
    "            content=log_line_dict['Content']\n",
    "            if skip_line(content):\n",
    "                raise ValueError(\"Line skipped\")\n",
    "            return content\n",
    "        except Exception as e:\n",
    "            return ''\n",
    "\n",
    "    starttime=datetime.datetime.now()\n",
    "\n",
    "    numb_var=NumbVar()\n",
    "\n",
    "    with open(log_file, 'r', encoding='utf8', errors='ignore') as input_file: #, buffering=1024*1024) as input_file:\n",
    "        i=0\n",
    "        for line in input_file: #.readline():\n",
    "            content=get_content_from_line(line)\n",
    "            if content=='':\n",
    "                continue\n",
    "            \n",
    "            template,id=numb_var.parse(content)\n",
    "            \n",
    "            if i % (lines_count // 100) == 0:\n",
    "                print(f\"Processed {i} lines ({i/lines_count*100:.2f}%), got {len(numb_var.get_templates_list())} templates\")\n",
    "                elapsed_time = datetime.datetime.now() - starttime\n",
    "                lines_per_second = i / elapsed_time.total_seconds()\n",
    "                print(f\"  Elapsed time: {elapsed_time}, Lines per second: {lines_per_second:.2f}\")\n",
    "            i+=1\n",
    "            \n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as output_file:\n",
    "            for tmpl in numb_var.get_templates_list():\n",
    "                output_file.write(f\"{tmpl}\\n\")\n",
    "\n",
    "    t_count=len(numb_var.get_templates_list())\n",
    "    print(f'Mined {t_count} templates')    \n",
    "\n",
    "#HDFS with 11175629 lines takes more than 2 min 40\n",
    "# for all 16 datasets it last approx. 9 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_of_dss = ['HDFS',\n",
    "                'Hadoop',\n",
    "                'Spark',\n",
    "                'Zookeeper',\n",
    "                'OpenStack',\n",
    "                'BGL',\n",
    "                'HPC',\n",
    "                'Thunderbird',\n",
    "                'Windows',\n",
    "                'Linux',\n",
    "                'Mac',\n",
    "                'Android',\n",
    "                'HealthApp',\n",
    "                'Apache',\n",
    "                'OpenSSH',\n",
    "                'Proxifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "templates_numb_var = {}\n",
    "for dataset, file_path in template_files.items():\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        line_count = sum(1 for _ in f)\n",
    "    templates_numb_var[dataset] = line_count\n",
    "templates_numb_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "templates_numb_var_df=pd.DataFrame.from_dict(templates_numb_var, orient='index', columns=['total_templates'])\n",
    "templates_numb_var_df['total_lines'] = [c['lines'] for _,c in config.items()]\n",
    "templates_numb_var_df['lines_per_template']= round(templates_numb_var_df['total_lines']/templates_numb_var_df['total_templates']).astype(int)\n",
    "templates_numb_var_df = templates_numb_var_df[['total_lines', 'total_templates', 'lines_per_template']]\n",
    "templates_numb_var_df = templates_numb_var_df.reindex(order_of_dss)\n",
    "templates_numb_var_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect 10.000 Lines each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "\n",
    "\n",
    "def skip_line(line:str)->bool:\n",
    "    if content is None:\n",
    "        return True\n",
    "    if line.strip() == '':\n",
    "        return True\n",
    "    if line.lower() in ['none', 'null', 'nan', 'n/a', 'na']:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "logs_10k={}\n",
    "\n",
    "for dataset, config in config.items():\n",
    "    output_file=os.path.join(OUT_DIR,OUTPUT_FILE_N_LINES.format(dataset=dataset, N=N, SEED=SEED))\n",
    "    logs_10k[dataset]=output_file\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"File {output_file} already exists, skipping\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    log_file = os.path.join(INPUT_DIR, config[\"log_file\"])\n",
    "    log_format:str=config[\"log_format\"]\n",
    "    headers: list[str]\n",
    "    format_regex: re.Pattern\n",
    "    headers, format_regex = generate_logformat_regex(log_format)\n",
    "\n",
    "    lines_count=config[\"lines\"]\n",
    "\n",
    "    print(f\"\\nProcessing {dataset} ({log_file})\")\n",
    "    random.seed(SEED)\n",
    "\n",
    "    # use dow while to fill up to N\n",
    "    lines_to_collect = N\n",
    "    log_content:list[tuple[str,int,str]]=[]\n",
    "    retries=0\n",
    "    runs=0\n",
    "\n",
    "    while True:\n",
    "        runs+=1\n",
    "        if len(log_content)>=N:\n",
    "            break\n",
    "\n",
    "        lines_to_collect = N-len(log_content)\n",
    "        selected_indices = dict.fromkeys(sorted(list(set(random.sample(range(lines_count), lines_to_collect)))),None)\n",
    "        \n",
    "        with open(log_file, 'r', encoding='utf8', errors='ignore') as input_file:\n",
    "            skipped_lines=0\n",
    "            for i,cur_line in enumerate(input_file):\n",
    "                if i not in selected_indices:\n",
    "                    continue\n",
    "                #content=''\n",
    "                try:\n",
    "                    match = format_regex.search(cur_line.strip())\n",
    "                    #message = [match.group(header) for header in headers]\n",
    "                    log_line_dict = {header:match.group(header) for header in headers}\n",
    "                    content=log_line_dict['Content']\n",
    "                    if skip_line(content):\n",
    "                        raise ValueError(\"Line skipped\")\n",
    "                    log_content.append((dataset,i,content))\n",
    "                except Exception as e:\n",
    "                    skipped_lines+=1\n",
    "            \n",
    "            retries+=skipped_lines\n",
    "    pickle.dump(log_content, open(output_file, \"wb\"))\n",
    "\n",
    "    print(f'{len(log_content)} lines collected')\n",
    "    print(f\"Retried {retries} lines in {runs} runs\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "examples_df=pd.DataFrame(columns=['Dataset','Line','Example'])\n",
    "for dataset, config in config.items():\n",
    "    file=logs_10k[dataset]\n",
    "    log_content=pickle.load(open(file, \"rb\"))\n",
    "    examples_df=pd.concat([examples_df,pd.DataFrame(log_content, columns=['Dataset','Line','Example'])])\n",
    "examples_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Templates on 10k Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.logparsing import NumbVar\n",
    "\n",
    "#mined_templates:list[tuple[str,str]]=[]\n",
    "mined_templates_count=0\n",
    "\n",
    "examples_df['Template'] = ''\n",
    "examples_df['ClusterId'] = ''\n",
    "\n",
    "templates_in_10k={}\n",
    "for dataset, config in config.items():\n",
    "    print(f\"Processing {dataset}\", end=' ')\n",
    "\n",
    "    dataset_examples_df:pd.DataFrame=examples_df[examples_df['Dataset']==dataset]\n",
    "    log_contents=dataset_examples_df['Example'].tolist()\n",
    "    \n",
    "    templates:list[tuple[str,int]]=[]\n",
    "    numb_var=NumbVar()\n",
    "    for content in log_contents:\n",
    "        template, id = numb_var.parse(content)\n",
    "        templates.append((template, id))\n",
    "    \n",
    "    examples_df.loc[examples_df['Dataset'] == dataset, ['Template', 'ClusterId']] = templates\n",
    "    \n",
    "    templates_count=max([i for _,i in templates])\n",
    "\n",
    "    templates_in_10k[dataset]=templates_count\n",
    "    print(f'Mined {templates_count} templates')\n",
    "    mined_templates_count+=templates_count\n",
    "\n",
    "print(f\"Total mined templates: {mined_templates_count}\")\n",
    "templates_numb_var_df['templates_in_10k']=templates_in_10k\n",
    "\n",
    "examples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates_numb_var_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Examples: {len(examples_df)}\")\n",
    "print(f\"Empty: {len(examples_df[examples_df['Example']==''])}\")\n",
    "\n",
    "# keep unique dataset, clusterid olnly\n",
    "examples_df_unique=examples_df.drop_duplicates(subset=['Dataset','ClusterId'])\n",
    "print(f\"Different Templates: {len(examples_df_unique)}\")\n",
    "examples_df_unique\n",
    "\n",
    "# drop empty examples\n",
    "examples_df_unique=examples_df_unique[examples_df_unique['Example']!='']\n",
    "print(f\"Without empty: {len(examples_df_unique)}\")\n",
    "\n",
    "# drop examples of string 'null' and 'nan'\n",
    "examples_df_unique=examples_df_unique[examples_df_unique['Example']!='null']\n",
    "examples_df_unique=examples_df_unique[examples_df_unique['Example']!='nan']\n",
    "print(f\"Without null/nan: {len(examples_df_unique)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(f\"Save to '{OUTPUT_FILE}'\")\n",
    "examples_df_unique.to_csv(OUTPUT_FILE, index=False)\n",
    "examples_df_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates_in_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates_numb_var_df['ratio_lines']=N/templates_numb_var_df['total_lines']\n",
    "templates_numb_var_df['ratio_templates']=templates_numb_var_df['templates_in_10k']/templates_numb_var_df['total_templates']\n",
    "templates_numb_var_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Bar width\n",
    "width = 0.35\n",
    "\n",
    "# X positions for the datasets\n",
    "x = np.arange(len(templates_numb_var_df.index))\n",
    "\n",
    "# Bar data\n",
    "y1 = templates_numb_var_df['ratio_lines']\n",
    "y2 = templates_numb_var_df['ratio_templates']\n",
    "\n",
    "# Plot bars\n",
    "ax.bar(x - width/2, y1, width, label='Ratio Lines')\n",
    "ax.bar(x + width/2, y2, width, label='Ratio Templates')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel('Dataset')\n",
    "ax.set_ylabel('Ratio')\n",
    "ax.set_title('Ratio Lines and Ratio Templates by Dataset')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(templates_numb_var_df.index, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Bar height\n",
    "height = 1.0 / 5\n",
    "\n",
    "# Y positions for the datasets\n",
    "y = np.arange(len(templates_numb_var_df.index))\n",
    "\n",
    "# Bar data\n",
    "x1 = templates_numb_var_df['ratio_lines']\n",
    "x2 = templates_numb_var_df['ratio_templates']\n",
    "x3 = templates_numb_var_df['total_lines']\n",
    "x4 = templates_numb_var_df['total_templates']\n",
    "\n",
    "# Plot bars\n",
    "ax.barh(y - height*2, x1, height, label='Ratio Lines')\n",
    "ax.barh(y-height, x2, height, label='Ratio Templates')\n",
    "\n",
    "# Add total lines as a secondary axis\n",
    "ax2 = ax.twiny()\n",
    "ax2.barh(y + height, x3, height, color='gray', alpha=0.3, label='Total Lines')\n",
    "ax2.barh(y+height*2,x4, height, color='green', alpha=0.3, label='Total Templates')\n",
    "\n",
    "# Add values to the end of the bars\n",
    "for i, (val1, val2, val3, val4) in enumerate(zip(x1, x2, x3, x4)):\n",
    "    ax.text(val1, y[i] - height*2, f'{val1*100:.1f}%', va='center', ha='left', fontsize=7)\n",
    "    ax.text(val2, y[i] - height, f'{val2*100:.1f}%', va='center', ha='left', fontsize=7)\n",
    "    ax2.text(val3, y[i] + height, f'{val3:,}', va='center', ha='left', fontsize=7)\n",
    "    ax2.text(val4, y[i] + height*2, f'{val4:,}', va='center', ha='left', fontsize=7)\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel('Ratio')\n",
    "ax.set_ylabel('Dataset')\n",
    "ax.set_title('Horizontal Bar Chart: Ratio Lines, Ratio Templates, and Total Lines by Dataset')\n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels(templates_numb_var_df.index)\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "ax2.set_xlabel('Total Lines')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data for the pie chart\n",
    "labels = templates_numb_var_df.index\n",
    "sizes = templates_numb_var_df['total_lines']\n",
    "colors = plt.cm.tab20.colors  # Use a colormap for better visualization\n",
    "\n",
    "# Plotting the pie chart\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140, colors=colors)\n",
    "ax.set_title('Distribution of Total Lines Across Datasets')\n",
    "\n",
    "# Add a table next to the pie chart\n",
    "divider = make_axes_locatable(ax)\n",
    "ax_table = divider.append_axes(\"right\", size=\"40%\", pad=0.1)\n",
    "\n",
    "# Prepare data for the table\n",
    "table_data = templates_numb_var_df[['total_lines', 'total_templates']].reset_index()\n",
    "table_data.columns = ['Dataset', 'Total Lines', 'Total Templates']\n",
    "\n",
    "# Hide the axis for the table\n",
    "ax_table.axis('off')\n",
    "\n",
    "# Create the table\n",
    "table = ax_table.table(cellText=table_data.values, colLabels=table_data.columns, loc='center', cellLoc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.auto_set_column_width(col=list(range(len(table_data.columns))))\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data for the pie chart\n",
    "labels = templates_numb_var_df.index\n",
    "sizes = templates_numb_var_df['total_templates']\n",
    "colors = plt.cm.tab20.colors  # Use a colormap for better visualization\n",
    "\n",
    "# Plotting the pie chart\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140, colors=colors)\n",
    "ax.set_title('Distribution of Total Templates Across Datasets')\n",
    "\n",
    "# Add a table next to the pie chart\n",
    "divider = make_axes_locatable(ax)\n",
    "ax_table = divider.append_axes(\"right\", size=\"40%\", pad=0.1)\n",
    "\n",
    "# Prepare data for the table\n",
    "table_data = templates_numb_var_df[['total_lines', 'total_templates']].reset_index()\n",
    "table_data.columns = ['Dataset', 'Total Lines', 'Total Templates']\n",
    "\n",
    "# Hide the axis for the table\n",
    "ax_table.axis('off')\n",
    "\n",
    "# Create the table\n",
    "table = ax_table.table(cellText=table_data.values, colLabels=table_data.columns, loc='center', cellLoc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.auto_set_column_width(col=list(range(len(table_data.columns))))\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data for the pie chart\n",
    "labels = templates_numb_var_df.index\n",
    "sizes = templates_numb_var_df['templates_in_10k']\n",
    "colors = plt.cm.tab20.colors  # Use a colormap for better visualization\n",
    "\n",
    "# Plotting the pie chart\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140, colors=colors)\n",
    "ax.set_title('Distribution of Mined Templates (10k each)')\n",
    "\n",
    "# Add a table next to the pie chart\n",
    "divider = make_axes_locatable(ax)\n",
    "ax_table = divider.append_axes(\"right\", size=\"40%\", pad=0.1)\n",
    "\n",
    "# Prepare data for the table\n",
    "table_data = templates_numb_var_df['templates_in_10k'].reset_index()\n",
    "table_data.columns = ['Dataset', 'Templates in 10k']\n",
    "table_data.sort_values(by='Templates in 10k', ascending=False, inplace=True)\n",
    "\n",
    "# Hide the axis for the table\n",
    "ax_table.axis('off')\n",
    "\n",
    "# Create the table\n",
    "table = ax_table.table(cellText=table_data.values, colLabels=table_data.columns, loc='center', cellLoc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.auto_set_column_width(col=list(range(len(table_data.columns))))\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
