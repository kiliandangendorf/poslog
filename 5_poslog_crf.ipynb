{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERIC FIRST CELL FOR DEVELOPING A NEW METHODOLOGY\n",
    "\n",
    "# Define an expressive name for the methodology, that will be used to create the output directory\n",
    "# The complete path to the output directory will be provided in OUT_DIR\n",
    "METHODOLOGY_NAME='pos_log/'\n",
    "\n",
    "import os\n",
    "OUT_DIR = os.path.relpath(os.path.join(os.getcwd(), 'out', METHODOLOGY_NAME))\n",
    "if not os.path.exists(OUT_DIR):\n",
    "        os.makedirs(OUT_DIR)\n",
    "print(f\"Current output directory: '{OUT_DIR}'\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS_LOG_MODEL='pos_log_upos_crf_10k_model_pre_manual'\n",
    "\n",
    "# # Precons\n",
    "# INPUT_FILE=os.path.join(OUT_DIR, '..','numb_var','examples_10000_each_seed-42_numb_var.csv')\n",
    "\n",
    "# # if file does not exist, throw an error\n",
    "# if not os.path.exists(INPUT_FILE):\n",
    "#     precon='1_templates_collect_numb_var.ipynb'\n",
    "#     raise FileNotFoundError(f\"File '{INPUT_FILE}' not found. Run '{precon}' first.\")\n",
    "\n",
    "# OUTPUT_FILE=os.path.join(OUT_DIR, 'examples_tagged_upos.csv')\n",
    "# print(f'Output file: {OUTPUT_FILE}')\n",
    "\n",
    "# RANDOM_SAMPLE_SIZE=100\n",
    "# RANDOM_SAMPLE_SEED=42\n",
    "# OUTPUT_FILE_RANDOM_SAMPLE=os.path.join(OUT_DIR, f'examples_tagged_{RANDOM_SAMPLE_SIZE}_random_sample_seed-{RANDOM_SAMPLE_SEED}_upos.csv')\n",
    "# print(f'Output file: {OUTPUT_FILE_RANDOM_SAMPLE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# # columns: Dataset,Line,Example,Template,ClusterId\n",
    "# example_df=pd.read_csv(INPUT_FILE)\n",
    "# example_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagged_sents=[]\n",
    "# for word_i in full_tagged_indices:\n",
    "#     tokens=examples_splitted[word_i]\n",
    "#     tags=majorities[word_i]\n",
    "#     tagged_sents.append(list(zip(tokens,tags)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train First CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp.pos import PosLogCRF\n",
    "crf=PosLogCRF(model_path=POS_LOG_MODEL)\n",
    "crf.train_from_tagged_sents(tagged_sents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now tag with all taggers not fully corrected lines and evaluate while skipping None Values in Tags\n",
    "# This should give an Comparison of PosLog and Taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are all lines and their counted majority containing Nones\n",
    "majorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line indices that was not trained on\n",
    "len(ragged_tagged_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragged_lines = [examples_splitted[i] for i in ragged_tagged_indices]\n",
    "ragged_majorities = [majorities[i] for i in ragged_tagged_indices]\n",
    "X=ragged_lines\n",
    "y=ragged_majorities\n",
    "#ragged_zipped = [list(zip(line, majority)) for line, majority in zip(ragged_lines, ragged_majorities)]\n",
    "#ragged_zipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=[crf.predict(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_none_values_from_ys(y, y_pred):\n",
    "    yas=[]\n",
    "    yps=[]\n",
    "    for i in range(len(y)):\n",
    "        ya=[]\n",
    "        yp=[]\n",
    "        for j in range(len(y[i])):\n",
    "            #if y[i][j] is not None:# and y[i][j]!='PROMETEUS':\n",
    "            if y[i][j] is not None and not y[i][j].startswith('PROMETEUS'):\n",
    "                ya.append(y[i][j])\n",
    "                yp.append(y_pred[i][j])\n",
    "        yas.append(ya)\n",
    "        yps.append(yp)\n",
    "    return yas, yps\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def print_accuracy(y, y_pred, report=False)->float:\n",
    "    yas, yps = remove_none_values_from_ys(y, y_pred)\n",
    "    y_t=[tag for tags in yas for tag in tags]\n",
    "    y_p=[tag for tags in yps for tag in tags]\n",
    "\n",
    "    acc=accuracy_score(y_t, y_p)\n",
    "    print(acc)\n",
    "    if report:\n",
    "        print(classification_report(y_t, y_p))\n",
    "    return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def tag_and_meas_time(tagger:AbstractPosTagger, X:list[list[str]])->tuple[list[list[str]], float]:\n",
    "    time_start = time.time()\n",
    "    y_pred=[tagger.pos_tag(x) for x in X]\n",
    "    elapsed_time=time.time()-time_start\n",
    "    return y_pred, elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_and_evaluate(tagger:AbstractPosTagger, X:list[list[str]], y:list[list[str]], last_pred:tuple[list[list[str]],float]|None=None, report=False)->list[list[str]]:\n",
    "    print(tagger.__class__.__name__)\n",
    "\n",
    "    if last_pred is None:\n",
    "        y_pred,elapsed_time=tag_and_meas_time(tagger, X)\n",
    "    else:\n",
    "        y_pred,elapsed_time=last_pred\n",
    "\n",
    "    acc=print_accuracy(y, y_pred, report)\n",
    "    lines=len(X)\n",
    "    tokens=sum([len(x) for x in X])\n",
    "    print(f\"Lines: {lines}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Elapsed time: {elapsed_time}\")\n",
    "    print(f\"Time per 1000 lines: {elapsed_time/lines*1000}\")\n",
    "    print(f\"Time per 1000.000 tokens: {elapsed_time/tokens*1000000}\\n\")\n",
    "    return (y_pred, elapsed_time, acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tag_and_evaluate(crf, X, y, report=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poslog=PosLogCRF(model_path=POS_LOG_MODEL)\n",
    "x=tag_and_evaluate(poslog, X, y, report=True)\n",
    "#x=tag_and_evaluate(crf, X, y, report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nltk=tag_and_evaluate(nltk_tagger, X, y)\n",
    "y_pred_stanza=tag_and_evaluate(stanza_tagger, X, y)\n",
    "y_pred_spacy=tag_and_evaluate(spacy_tagger, X, y)\n",
    "y_pred_hanta=tag_and_evaluate(hanta_tagger, X, y)\n",
    "y_pred_treetagger=tag_and_evaluate(treetagger_tagger, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poslog=PosLogCRF(model_path=POS_LOG_MODEL)\n",
    "# x=tag_and_evaluate(poslog, X, y, report=True)\n",
    "# #x=tag_and_evaluate(crf, X, y, report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = example_df['Tokens'].tolist()[9]\n",
    "print(t)\n",
    "\n",
    "\n",
    "for name, tagger_func in tagger.items():\n",
    "    if name=='pos_log':\n",
    "        tagger_func=poslog.pos_tag\n",
    "    #pos_tags = poslog.pos_tag(tokenizer.tokenize(s))\n",
    "    #t=tokenizer.tokenize(s)\n",
    "    pos_tags = tagger_func(t)\n",
    "    #pos_tags = [tag for token, tag in pos_tags]\n",
    "    print(name, list(zip(t, pos_tags)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_accuracy(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ragged_tagged_indices)\n",
    "print(full_tagged_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_ds_indices={i:None for i in example_df[example_df['Dataset']=='HDFS'].index.tolist()}\n",
    "len(only_ds_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp.pos import PosLogCRF\n",
    "\n",
    "def exclude_ds_and_train(dataset:str):\n",
    "    datasets=example_df['Dataset'].unique().tolist()\n",
    "\n",
    "    all_but_ds_indices={i:None for i in example_df[example_df['Dataset']!=dataset].index.tolist()}\n",
    "    only_ds_indices={i:None for i in example_df[example_df['Dataset']==dataset].index.tolist()}\n",
    "\n",
    "    train_datasets=[d for d in datasets if d!=dataset]\n",
    "    print(f\"Training on {train_datasets} ({len(all_but_ds_indices)} lines) and excluding {dataset} ({len(only_ds_indices)} lines)\")\n",
    "\n",
    "    tagged_sents_ex=[]\n",
    "    for i in full_tagged_indices:\n",
    "        if i in only_ds_indices:\n",
    "            continue\n",
    "        tokens=examples_splitted[i]\n",
    "        tags=majorities[i]\n",
    "        tagged_sents_ex.append(list(zip(tokens,tags)))\n",
    "    pos_log_ex=PosLogCRF(model_path='ex_'+dataset+'_'+POS_LOG_MODEL)\n",
    "    pos_log_ex.train_from_tagged_sents(tagged_sents_ex)\n",
    "\n",
    "    y_ex=[]\n",
    "    X_ex=[]\n",
    "    for i in ragged_tagged_indices:\n",
    "        if i in all_but_ds_indices:\n",
    "            continue\n",
    "        X_ex.append(examples_splitted[i])\n",
    "        y_ex.append(majorities[i])\n",
    "\n",
    "    print(f\"Trained on {len(tagged_sents_ex)} lines, evaluating on {len(X_ex)} lines\")\n",
    "    x_ex=tag_and_evaluate(pos_log_ex, X_ex, y_ex)#, report=True)\n",
    "\n",
    "    # y_ex=[]\n",
    "    # X_ex=[]\n",
    "    # for i in ragged_tagged_indices:\n",
    "    #     X_ex.append(examples_splitted[i])\n",
    "    #     y_ex.append(majorities[i])\n",
    "    # print(f\"All datasets lines: {len(X_ex)}\")\n",
    "    # x_all=tag_and_evaluate(pos_log_ex, X_ex, y_ex)#, report=True)\n",
    "    return x_ex\n",
    "\n",
    "x=exclude_ds_and_train('HDFS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp.pos import PosLogCRF\n",
    "\n",
    "datasets=example_df['Dataset'].unique().tolist()\n",
    "\n",
    "exclude_comparisons=pd.DataFrame(datasets, columns=['Dataset'])\n",
    "\n",
    "accs=[]\n",
    "for dataset in datasets:\n",
    "    print(f\"\\nDataset: {dataset}\")\n",
    "    x=exclude_ds_and_train(dataset)\n",
    "    accs.append(x[2])\n",
    "exclude_comparisons['poslog'] = accs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tagger_objects=[nltk_tagger, stanza_tagger, spacy_tagger, hanta_tagger, treetagger_tagger]\n",
    "for tagger_x in tagger_objects:\n",
    "    accs=[]\n",
    "    for dataset in datasets:\n",
    "        #tagger_x=tagger\n",
    "\n",
    "        print(f\"\\nDataset: {dataset}\")\n",
    "        datasets=example_df['Dataset'].unique().tolist()\n",
    "\n",
    "        all_but_ds_indices={i:None for i in example_df[example_df['Dataset']!=dataset].index.tolist()}\n",
    "        only_ds_indices={i:None for i in example_df[example_df['Dataset']==dataset].index.tolist()}\n",
    "\n",
    "        train_datasets=[d for d in datasets if d!=dataset]\n",
    "        print(f\"Training on {train_datasets} ({len(all_but_ds_indices)} lines) and excluding {dataset} ({len(only_ds_indices)} lines)\")\n",
    "\n",
    "        tagged_sents_ex=[]\n",
    "        for word_i in full_tagged_indices:\n",
    "            if word_i in only_ds_indices:\n",
    "                continue\n",
    "            tokens=examples_splitted[word_i]\n",
    "            tags=majorities[word_i]\n",
    "            tagged_sents_ex.append(list(zip(tokens,tags)))\n",
    "\n",
    "        y_ex=[]\n",
    "        X_ex=[]\n",
    "        for word_i in ragged_tagged_indices:\n",
    "            if word_i in all_but_ds_indices:\n",
    "                continue\n",
    "            X_ex.append(examples_splitted[word_i])\n",
    "            y_ex.append(majorities[word_i])\n",
    "\n",
    "        print(f\"Trained on {len(tagged_sents_ex)} lines, evaluating on {len(X_ex)} lines\")\n",
    "        x_ex=tag_and_evaluate(tagger_x, X_ex, y_ex)#, report=True)\n",
    "\n",
    "        accs.append(x_ex[2])\n",
    "    exclude_comparisons[tagger_x.__class__.__name__] = accs\n",
    "\n",
    "    # y_ex=[]\n",
    "    # X_ex=[]\n",
    "    # for i in ragged_tagged_indices:\n",
    "    #     X_ex.append(examples_splitted[i])\n",
    "    #     y_ex.append(majorities[i])\n",
    "    # print(f\"All datasets lines: {len(X_ex)}\")\n",
    "    # x_all=tag_and_evaluate(pos_log_ex, X_ex, y_ex)#, report=True)\n",
    "    #return x_ex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accs=[]\n",
    "for dataset in datasets:\n",
    "    tagger_x=poslog\n",
    "\n",
    "    print(f\"\\nDataset: {dataset}\")\n",
    "    datasets=example_df['Dataset'].unique().tolist()\n",
    "\n",
    "    all_but_ds_indices={i:None for i in example_df[example_df['Dataset']!=dataset].index.tolist()}\n",
    "    only_ds_indices={i:None for i in example_df[example_df['Dataset']==dataset].index.tolist()}\n",
    "\n",
    "    train_datasets=[d for d in datasets if d!=dataset]\n",
    "    print(f\"Training on {train_datasets} ({len(all_but_ds_indices)} lines) and excluding {dataset} ({len(only_ds_indices)} lines)\")\n",
    "\n",
    "    tagged_sents_ex=[]\n",
    "    for word_i in full_tagged_indices:\n",
    "        if word_i in only_ds_indices:\n",
    "            continue\n",
    "        tokens=examples_splitted[word_i]\n",
    "        tags=majorities[word_i]\n",
    "        tagged_sents_ex.append(list(zip(tokens,tags)))\n",
    "\n",
    "    y_ex=[]\n",
    "    X_ex=[]\n",
    "    for word_i in ragged_tagged_indices:\n",
    "        if word_i in all_but_ds_indices:\n",
    "            continue\n",
    "        X_ex.append(examples_splitted[word_i])\n",
    "        y_ex.append(majorities[word_i])\n",
    "\n",
    "    print(f\"Trained on {len(tagged_sents_ex)} lines, evaluating on {len(X_ex)} lines\")\n",
    "    x_ex=tag_and_evaluate(tagger_x, X_ex, y_ex)#, report=True)\n",
    "\n",
    "    accs.append(x_ex[2])\n",
    "\n",
    "exclude_comparisons[crf.__class__.__name__] = accs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclude_comparisons = exclude_comparisons.drop(index='average', errors='ignore')\n",
    "exclude_comparisons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exclude_comparisons.loc['average'] = exclude_comparisons.drop(columns=['Dataset']).mean(axis=0)\n",
    "exclude_comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add PosLog trained on all datasets (not excluding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all parity tokens\n",
    "c=0\n",
    "l=[]\n",
    "for tokens,tc in zip(examples_splitted,tag_comparisons):\n",
    "    for word_i,maj in enumerate(tc.majority):\n",
    "        if tc.confidence[word_i]<=0.6:\n",
    "            word_kind=known_words_det.kind_of_known_word(tokens[word_i])\n",
    "            if word_kind!=WordKind.UNKNOWN:\n",
    "                continue\n",
    "            c+=1\n",
    "            l.append(tokens[word_i])\n",
    "            print(f\"Token: {tokens[word_i]} Majority: {tc.majority[word_i]} Minority: {tc.minority[word_i]}\")\n",
    "print(f\"Total: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s='i have to walk'\n",
    "t=tokenizer.tokenize(s)\n",
    "for name, func in tagger.items():\n",
    "    print(f\"{name}: {func(t)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(l)))\n",
    "sorted(list(set(l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matcher.token_class('lol'))\n",
    "for t in sorted(list(set(l))):\n",
    "    print(t,matcher.token_class(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s='his hostname was Noname'\n",
    "for name, func in tagger.items():\n",
    "    print(func(tokenizer.tokenize(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if Prometeus said it was PUNCTUATION, then correct all tags to PUNCT with confidence 1.0\n",
    "import string\n",
    "import re\n",
    "def override_punctuation_tags(tokens:list[str], tag_comparison:TagComparison)->TagComparison:\n",
    "    for i,token in enumerate(tokens):\n",
    "        #if token in string.punctuation:\n",
    "        # since string.punctuation is defined as raw string, we can use it directly without matching e.g. '.*'\n",
    "        if re.fullmatch(r'['+string.punctuation+']+', token):\n",
    "            tag_comparison.majority[i]='PUNCT'\n",
    "            tag_comparison.confidence[i]=1.0\n",
    "            tag_comparison.minority[i]={}\n",
    "    return tag_comparison\n",
    "\n",
    "tag_comparisons_masked=[override_punctuation_tags(tokens, tc) for tokens,tc in zip(masked_examples, tag_comparisons_masked)]\n",
    "print('Masked')\n",
    "print_stats_on_tag_comparison(tag_comparisons_masked)\n",
    "print()\n",
    "print()\n",
    "print('Unmasked')\n",
    "tag_comparisons_unmasked=[override_punctuation_tags(tokens, tc) for tokens,tc in zip(masked_examples, tag_comparisons_unmasked)]\n",
    "print_stats_on_tag_comparison(tag_comparisons_unmasked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter example_df for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to df\n",
    "example_df['TagComparison']=tag_comparisons_unmasked\n",
    "example_df['TagComparisonMasked']=tag_comparisons_masked\n",
    "#write file\n",
    "example_df.to_csv(OUTPUT_FILE, index=False)\n",
    "example_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s='hostname'\n",
    "s='?'\n",
    "mask=masker.get_token_class(s)\n",
    "print(f'Masked: {mask}')\n",
    "print(f'Is masked? {masker.is_masked_token(mask)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# UPOS\n",
    "######\n",
    "# for nltk we need our own mapping or use the \"repaired\" one\n",
    "# stanza does full upos :)\n",
    "# spacy looks good, too\n",
    "\n",
    "# hanta we need to repair the mapping\n",
    "# treetagger we need our own mapping, but we can use \"repaired\" nltk's mapping again\n",
    "\n",
    "######\n",
    "# PTB\n",
    "######\n",
    "# nltk does ptb\n",
    "# stanza does ptb (most likely)\n",
    "# spacy does ptb (most likely)\n",
    "# hante we need to build brown2ptb mapping\n",
    "# tree tagger uses ptb, but we may have to fix some tags\n",
    "\n",
    "\n",
    "s=\"It's not easy to find words, because it needs 10 $ at the door which had died. And Reagen has died, too. I give up trying to get a particle. But hey, there it is ;) Wow!\"\n",
    "#s='session opened for user cyrus by (uid=0)'\n",
    "print(nltk_tagger.pos_tag_ptb(tokenizer.tokenize(s)))\n",
    "print(nltk_tagger.pos_tag_upos(tokenizer.tokenize(s)))\n",
    "print(stanza_tagger.pos_tag_upos(tokenizer.tokenize(s)))\n",
    "print(spacy_tagger.pos_tag_upos(tokenizer.tokenize(s)))\n",
    "print(hanta_tagger.pos_tag_upos(tokenizer.tokenize(s)))\n",
    "print(treetagger_tagger.pos_tag_upos(tokenizer.tokenize(s)))\n",
    "\n",
    "print(tokenizer.tokenize(s))\n",
    "ttts=sorted(set(spacy_tagger.pos_tag_upos(tokenizer.tokenize(s))))\n",
    "print(len(ttts))\n",
    "print(ttts)\n",
    "\n",
    "tttsn=sorted(set(hanta_tagger.pos_tag_upos(tokenizer.tokenize(s))))\n",
    "print(len(tttsn))\n",
    "print(tttsn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "s=\"It's `really' \\\"not\\\" easy -- to ``find'' 100 words rather short than that, because it needs 10 $ at the door which had died. And Reagen has died, too. I give up trying to get a particle. But hey, there it is ;) Wow!\"\n",
    "#s=' . '.join(list(string.punctuation))\n",
    "print(s)\n",
    "ts=tokenizer.tokenize(s)\n",
    "ts2=zip(ts, nltk_tagger.pos_tag_upos(ts), [t[2] for t in hanta_tagger.nlp.tag_sent(ts, taglevel=1)])\n",
    "ts2=zip(ts, treetagger_tagger.pos_tag_ptb(ts), [t[2] for t in hanta_tagger.nlp.tag_sent(ts, taglevel=1)])\n",
    "ts2=zip(ts, spacy_tagger.pos_tag_upos(ts), stanza_tagger.pos_tag_upos(ts), treetagger_tagger.pos_tag_upos(ts), #treetagger_tagger.pos_tag_ptb(ts), \n",
    "        hanta_tagger.pos_tag_upos(ts), nltk_tagger.pos_tag_upos(ts))\n",
    "#from treetaggerwrapper import Tag, NotTag, make_tags\n",
    "#tsss=treetagger_tagger.nlp.tag_text(ts, nosgmlsplit=True, notagurl=True, notagemail=True, notagip=True, notagdns=True, tagonly=True)\n",
    "#ts2=zip(ts, make_tags(tsss), [t[2] for t in hanta_tagger.nlp.tag_sent(ts, taglevel=1)])\n",
    "for t in ts2:\n",
    "    if t[1]!=t[2] or t[1]!=t[3] or t[2]!=t[3]:\n",
    "        print(t)\n",
    "\n",
    "#treetagger_tagger.nlp.tagbin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp.pos.tag_comparison import TagComparison, make_tag_comparison, print_stats_on_tag_comparison\n",
    "s=\"I (give) up {trying} to [get] get a particle. But hey, there it is ;) Wow!\"\n",
    "from string import punctuation\n",
    "s=' . '.join(list(punctuation))\n",
    "print(s)\n",
    "ts=tokenizer.tokenize(s)\n",
    "taggers_res={}\n",
    "print(f\"Tokens: {ts}\")\n",
    "\n",
    "\n",
    "TAGSET='upos'\n",
    "#TAGSET='ptb'\n",
    "for name, func in tagger.items():\n",
    "    taggers_res[name]=func(ts,TAGSET)\n",
    "    #print(f\"{name}: {taggers_res[name]}\")\n",
    "    print(f\"{name}: {taggers_res[name]}\")\n",
    "tc=make_tag_comparison(taggers_res, TAGSET)\n",
    "print(tc)\n",
    "for word_i in range(len(ts)):\n",
    "    tcc=tc.confidence[word_i]\n",
    "    if tcc<1.0:\n",
    "        print(f\"{ts[word_i]}: {tc.majority[word_i]}\")\n",
    "        print(f\"{ts[word_i]}: {tc.minority[word_i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c=Counter({'a':1, 'b':2, 'c':3})\n",
    "sorted(c.values(), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=\"{\\\"uid\\\":\\\"MsMC0pc\\\",\\\"token\\\":null,\\\"ma\\\":null,\\\"question\\\":[{\\\"questionId\\\":62377,\\\"answerId\\\":244003,\\\"answerText\\\":\\\"Pinte\\n\\\",\\\"answer\\\":1}],\\\"iframe\\\":true,\\\"ref\\\":\\\"www.dewezet.de\\\",\\\"ts\\\":$(date +%s)}\"\n",
    "a=\"{\\\"uid\\\":false,\\\"token\\\":null,\\\"ma\\\":null,\\\"question\\\":[{\\\"questionId\\\":62377,\\\"answerId\\\":244003,\\\"answerText\\\":\\\"Pinte\\n\\\",\\\"answer\\\":1}],\\\"iframe\\\":true,\\\"ref\\\":\\\"www.dewezet.de\\\",\\\"ts\\\":$(date +%s)}\"\n",
    "a=\"https://aktion.dewezet.de/umfrage/g4kyv/auswertung/?q=1&uid=Noh9J2K&iframe&ref=www.dewezet.de&adUnit1=&adUnit2=&pageIVWCat=0\"\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s='BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.10.6:50010 is added to blk_-1608999687919862906 size 91178'\n",
    "s='PacketResponder 0 for block blk_-3544583377289625738 terminating'\n",
    "s='Received block blk_-9073992586687739851 of size 11977 from /10.250.7.244'\n",
    "print(s)\n",
    "print(tokenizer.tokenize(s))\n",
    "for t_n, t in tagger.items():\n",
    "    print(t_n, t(tokenizer.tokenize(s)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
