{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERIC FIRST CELL FOR DEVELOPING A NEW METHODOLOGY\n",
    "\n",
    "# Define an expressive name for the methodology, that will be used to create the output directory\n",
    "# The complete path to the output directory will be provided in OUT_DIR\n",
    "METHODOLOGY_NAME='pos_log/'\n",
    "\n",
    "import os\n",
    "OUT_DIR = os.path.relpath(os.path.join(os.getcwd(), 'out', METHODOLOGY_NAME))\n",
    "if not os.path.exists(OUT_DIR):\n",
    "        os.makedirs(OUT_DIR)\n",
    "print(f\"Current output directory: '{OUT_DIR}'\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precons\n",
    "INPUT_FILE=os.path.join(OUT_DIR, '..','numb_var','examples_10000_each_seed-42_numb_var.csv')\n",
    "\n",
    "# if file does not exist, throw an error\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    precon='1_templates_collect_numb_var.ipynb'\n",
    "    raise FileNotFoundError(f\"File '{INPUT_FILE}' not found. Run '{precon}' first.\")\n",
    "\n",
    "OUTPUT_FILE=os.path.join(OUT_DIR, 'examples_tagged_upos.csv')\n",
    "print(f'Output file: {OUTPUT_FILE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# columns: Dataset,Line,Example,Template,ClusterId\n",
    "example_df=pd.read_csv(INPUT_FILE)\n",
    "example_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp import PrometeusTokenizer\n",
    "tokenizer=PrometeusTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df['Tokens']=example_df['Example'].apply(tokenizer.tokenize)\n",
    "example_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify Punctuation Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_punct_tokens(tokens:list[str])->list[str]:\n",
    "    for i, token in enumerate(tokens):\n",
    "        if len(token)==1:\n",
    "            if token in ['(', '[', '{']:\n",
    "                tokens[i]='('\n",
    "            elif token in [')', ']', '}']:\n",
    "                tokens[i]=')'\n",
    "            elif token in ['.',',',';',':','!','?']:\n",
    "                # esp. for PTB to generalize punctuation\n",
    "                tokens[i]='.'\n",
    "        # TODO: More of these?\n",
    "    return tokens\n",
    "\n",
    "example_df['Tokens']=example_df['Tokens'].apply(unify_punct_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PoS-Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell installes all taggers (if not already installed)\n",
    "# So the first run may take a while.\n",
    "\n",
    "from nlp.pos import AbstractPosTagger, NLTKPosTagger, SpacyPosTagger, StanzaPosTagger, HanTaPosTagger, TreeTaggerPosTagger\n",
    "nltk_tagger=NLTKPosTagger()\n",
    "stanza_tagger=StanzaPosTagger()\n",
    "spacy_tagger=SpacyPosTagger()\n",
    "hanta_tagger=HanTaPosTagger()\n",
    "treetagger_tagger=TreeTaggerPosTagger()\n",
    "\n",
    "tagger:dict[str,AbstractPosTagger]={}\n",
    "\n",
    "tagger['nltk']=nltk_tagger.pos_tag\n",
    "tagger['stanza']=stanza_tagger.pos_tag\n",
    "tagger['spacy']=spacy_tagger.pos_tag\n",
    "tagger['hanta']=hanta_tagger.pos_tag\n",
    "tagger['treetagger']=treetagger_tagger.pos_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# deactivate logging for treetagger (since there was a pipe error otherwise)\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "i=50\n",
    "for tagger_name, tagger_func in tagger.items():\n",
    "    print(f'{tagger_name} tags line {i}: {tagger_func(example_df[\"Tokens\"][i])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag with all POS-Taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column for each tagger\n",
    "import time\n",
    "\n",
    "for name, tagger_func in tagger.items():\n",
    "    print(f\"Tagging with {name}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    example_df[name]=example_df['Tokens'].apply(tagger_func)\n",
    "\n",
    "    print(f\"- Time taken: {time.time() - start_time:.2f} seconds\")\n",
    "example_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "example_df.to_csv(OUTPUT_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
